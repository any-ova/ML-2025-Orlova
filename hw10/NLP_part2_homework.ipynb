{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tjJocIf2SVzR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "RtEMuvDVSYBk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание\n",
        "\n",
        "1) Реализовать методы `greedy_sampling` и `generate` (1 балл)\n",
        "2) Реализовать метод `random_sampling` и поддержать его в `generate` (1 балл)\n",
        "3) Реализовать метод `_beam_search_generate` и поддержать его в `generate` (2 балла)\n",
        "4) Реализовать методы `apply_top_p`, `apply_top_k`, `apply_temperature` и поддержать их в `generate` (1 балл)  \n",
        "Все методы необходимо реализовать через векторные операции в torch/numpy везде где это возможно"
      ],
      "metadata": {
        "id": "IvgXC6YxSm12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    def __init__(self, model_name: str = \"gpt2\"):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def greedy_sampling(self, logits: torch.Tensor) -> int:\n",
        "        return int(torch.argmax(logits).item())\n",
        "\n",
        "    def random_sampling(self, logits: torch.Tensor) -> int:\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        token = torch.multinomial(probs, num_samples=1)\n",
        "        return int(token.item())\n",
        "\n",
        "    def _beam_search_generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int,\n",
        "        num_beams: int,\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 1.0\n",
        "    ) -> str:\n",
        "        eos_id = self.tokenizer.eos_token_id\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
        "        seq_len = input_ids.shape[-1]\n",
        "        beams = [{\n",
        "            \"input_ids\": input_ids,\n",
        "            \"score\": 0.0,\n",
        "            \"done\": False\n",
        "        }]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _step in range(max_length - seq_len):\n",
        "                all_candidates = []\n",
        "                if all(b[\"done\"] for b in beams):\n",
        "                    break\n",
        "\n",
        "                for b in beams:\n",
        "                    if b[\"done\"]:\n",
        "                        all_candidates.append(b)\n",
        "                        continue\n",
        "\n",
        "                    outputs = self.model(b[\"input_ids\"])\n",
        "                    logits = outputs.logits[:, -1, :].squeeze(0)\n",
        "\n",
        "                    logits = self.apply_temperature(logits, temperature)\n",
        "                    if top_k and top_k > 0:\n",
        "                        logits = self._apply_top_k(logits, top_k)\n",
        "                    if top_p is not None and top_p < 1.0:\n",
        "                        logits = self._apply_top_p(logits, top_p)\n",
        "\n",
        "                    log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "\n",
        "                    k = min(num_beams, self.vocab_size)\n",
        "                    topk_vals, topk_idx = torch.topk(log_probs, k=k)\n",
        "\n",
        "                    for val, idx in zip(topk_vals.tolist(), topk_idx.tolist()):\n",
        "                        new_input_ids = torch.cat([b[\"input_ids\"], torch.tensor([[idx]], device=self.device)], dim=1)\n",
        "                        new_score = b[\"score\"] + float(val)\n",
        "                        done = (idx == eos_id)\n",
        "                        all_candidates.append({\n",
        "                            \"input_ids\": new_input_ids,\n",
        "                            \"score\": new_score,\n",
        "                            \"done\": done\n",
        "                        })\n",
        "\n",
        "                all_candidates.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "                beams = all_candidates[:num_beams]\n",
        "\n",
        "            finished = [b for b in beams if b[\"done\"]]\n",
        "            best = max(finished, key=lambda x: x[\"score\"]) if finished else max(beams, key=lambda x: x[\"score\"])\n",
        "\n",
        "            output_ids = best[\"input_ids\"].squeeze(0).tolist()\n",
        "            return self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "    def apply_temperature(self, logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
        "        if temperature is None or temperature == 1.0:\n",
        "            return logits\n",
        "        if temperature <= 0.0:\n",
        "            raise ValueError(\"температура выше > 0\")\n",
        "        return logits / temperature\n",
        "\n",
        "    def _apply_top_p(self, logits: torch.Tensor, top_p: float = 1.0) -> torch.Tensor:\n",
        "        if top_p is None or top_p >= 1.0:\n",
        "            return logits\n",
        "        if top_p <= 0.0:\n",
        "            return torch.full_like(logits, float(\"-inf\"))\n",
        "\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "        cutoff_mask = cumulative_probs > top_p\n",
        "        cutoff_mask[..., 0] = False\n",
        "\n",
        "        sorted_logits[cutoff_mask] = float(\"-inf\")\n",
        "        out_logits = torch.full_like(logits, float(\"-inf\"))\n",
        "        out_logits[sorted_indices] = sorted_logits\n",
        "        return out_logits\n",
        "\n",
        "    def _apply_top_k(self, logits: torch.Tensor, top_k: int = None) -> torch.Tensor:\n",
        "        if top_k is None or top_k <= 0 or top_k >= logits.size(-1):\n",
        "            return logits\n",
        "        topk_vals, topk_idx = torch.topk(logits, k=top_k)\n",
        "        mask = torch.full_like(logits, float(\"-inf\"))\n",
        "        mask[topk_idx] = logits[topk_idx]\n",
        "        return mask\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 50,\n",
        "        strategy: str = \"greedy\",\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 1.0,\n",
        "        num_beams: int = 3\n",
        "    ) -> str:\n",
        "\n",
        "        strategy = strategy.lower()\n",
        "        if strategy not in {\"greedy\", \"random\", \"beam\"}:\n",
        "            raise ValueError(\"неясное значение\")\n",
        "\n",
        "        if strategy == \"beam\":\n",
        "            return self._beam_search_generate(prompt, max_length, num_beams,\n",
        "                                              temperature=temperature, top_k=top_k, top_p=top_p)\n",
        "\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
        "        eos_id = self.tokenizer.eos_token_id\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length - input_ids.shape[-1]):\n",
        "                outputs = self.model(input_ids)\n",
        "                logits = outputs.logits[:, -1, :].squeeze(0)\n",
        "                logits = self.apply_temperature(logits, temperature)\n",
        "                if top_k and top_k > 0:\n",
        "                    logits = self._apply_top_k(logits, top_k)\n",
        "                if top_p is not None and top_p < 1.0:\n",
        "                    logits = self._apply_top_p(logits, top_p)\n",
        "\n",
        "                if strategy == \"greedy\":\n",
        "                    next_token_id = self.greedy_sampling(logits)\n",
        "                else:\n",
        "                    next_token_id = self.random_sampling(logits)\n",
        "\n",
        "                next_token = torch.tensor([[next_token_id]], device=self.device)\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "                if next_token_id == eos_id:\n",
        "                    break\n",
        "\n",
        "        return self.tokenizer.decode(input_ids.squeeze(0).tolist(), skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "JToKeNj7SYbx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = Model(\"gpt2\")\n",
        "\n",
        "prompt = \"Once upon a time\"\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"\\nGreedy:\")\n",
        "print(m.generate(prompt, max_length=40, strategy=\"greedy\"))\n",
        "\n",
        "print(\"\\nRandom (temperature=1.0):\")\n",
        "print(m.generate(prompt, max_length=40, strategy=\"random\", temperature=1.0))\n",
        "\n",
        "print(\"\\nRandom (temperature=0.7, top_k=50):\")\n",
        "print(m.generate(prompt, max_length=40, strategy=\"random\", temperature=0.7, top_k=50))\n",
        "\n",
        "print(\"\\nBeam search (num_beams=4):\")\n",
        "print(m.generate(prompt, max_length=40, strategy=\"beam\", num_beams=4, temperature=1.0, top_k=50))"
      ],
      "metadata": {
        "id": "aNUHC3UmSYd-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad9d2870-c8c5-44b7-875d-8e771321ea77"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Once upon a time\n",
            "\n",
            "Greedy:\n",
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a\n",
            "\n",
            "Random (temperature=1.0):\n",
            "Once upon a time, there was a furnace at Charles Town Hall, Zurich, to brew electricity from and would supply power. The fuel was pumped off tightly inside the building, and its Salernoelve\n",
            "\n",
            "Random (temperature=0.7, top_k=50):\n",
            "Once upon a time, there was a long, long time before the gods could be brought to order, and the people of the world were divided into many sects. Then came a time when the gods\n",
            "\n",
            "Beam search (num_beams=4):\n",
            "Once upon a time, the world was filled with the sounds of the sun and the moon, the sounds of the wind, the sounds of the waves, the sounds of the waves, the sounds of\n"
          ]
        }
      ]
    }
  ]
}